{
  "MULAN": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "MULAN uses a pre-trained sequence encoder combined with a separate structure adapter module, indicating modular design."
        },
        "transferability": {
          "score": 2,
          "note": "The model adapts across multiple protein-related tasks like function prediction and secondary structure prediction."
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "MULAN explicitly integrates both protein sequence and angle-based structure information."
        },
        "structural_awareness": {
          "score": 2,
          "note": "Structure adapter processes structural angle information to enhance protein representation."
        }
      }
    },
    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 1,
          "note": "The OpenReview paper does not mention released code explicitly. No GitHub link found yet."
        },
        "documentation_quality": {
          "score": 1,
          "note": "Since no code repository is found, documentation cannot be evaluated; partial score given assuming typical OpenReview standards."
        },
        "setup_ease": {
          "score": 1,
          "note": "Setup instructions are not available; assumed partial based on the complexity of architecture described."
        }
      }
    },
    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 2,
          "note": "Combines pre-trained components instead of building an excessively large model; efficient modular reuse."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Runtime scalability not benchmarked or discussed explicitly in the paper."
        }
      }
    },
    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Output interpretability (like attention visualization) not explicitly discussed."
        },
        "task_alignment": {
          "score": 2,
          "note": "Directly aligns outputs to practical biological tasks (function prediction, secondary structure prediction, molecular interaction prediction)."
        }
      }
    }
  },

  "PROCYON": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "Architecture integrates an LLM (Llama-3), a protein sequence encoder (ESM), a structure encoder (GearNet), retrieval projectors, and multimodal fusion modules; explicitly modular and co-trained. :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
          "score": 2,
          "note": "Demonstrated zero-shot task transfer to new phenotype descriptions, unseen phenotypes, predicting domain-level drug binding, peptide binding (after small finetune), and genetic variant effects. :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Supports interleaved modalities: sequence, structure, protein domains, peptides, small molecule structures, and text in a unified prompt. :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
          "score": 2,
          "note": "Uses GearNet structural encoder; retrieves structural domain-level functions; performs structural reasoning such as drug-binding domain identification and mutation phenotype modeling. :contentReference[oaicite:4]{index=4}"
        },
        "biological_scope_multiscale": {
          "score": 2,
          "note": "Covers 5 knowledge domains: molecular functions, diseases, therapeutics, domains, and molecular interactions; models multiscale phenotypes from molecular to organism-level. :contentReference[oaicite:5]{index=5}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 1,
          "note": "Paper states that training datasets, training code, inference code, and pretrained models are publicly available, but no GitHub link is included in the PDF text. Score = 1 until explicit link is provided. :contentReference[oaicite:6]{index=6}"
        },
        "documentation_quality": {
          "score": 0,
          "note": "Paper does not describe documentation, tutorials, model cards, examples, or instructions; therefore cannot be confirmed. :contentReference[oaicite:7]{index=7}"
        },
        "setup_ease": {
          "score": 1,
          "note": "Architecture is extremely large (11B parameters) and requires multimodal encoders; paper does not describe installation or environment setup. Partial score only for stating that pretrained models are made available. :contentReference[oaicite:8]{index=8}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 0,
          "note": "Model is 11B parameters and integrates multiple encoders; no claims of parameter-efficient design or compression. :contentReference[oaicite:9]{index=9}"
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Paper provides performance benchmarks but does not present runtime scaling experiments or computational profiling; partial evidence from retrieval latency and inference modules. :contentReference[oaicite:10]{index=10}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 2,
          "note": "Provides interpretable outputs: free-text phenotype generation, comparison with known biological functions, variant impact explanations, and domain-level retrieval visualizations. Includes t-SNE embedding structure shifts and self-QA filtering. :contentReference[oaicite:11]{index=11}"
        },
        "task_alignment": {
          "score": 2,
          "note": "Directly aligned with tasks such as contextual protein retrieval, phenotype QA, domain binding, peptide binding, disease-specific drug-target reasoning, and variant phenotype mapping. :contentReference[oaicite:12]{index=12}"
        },
        "multiscale_output_relevance": {
          "score": 2,
          "note": "Outputs are relevant across molecular, cellular, therapeutic, and disease levels; supports pleiotropic and compositional phenotype modeling. :contentReference[oaicite:13]{index=13}"
        }
      }
    }
  },

  "EvoLa": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "EvoLa combines multiple modules: MSATrans, EvoTrans, EvoShare, an FDA module, and optional protein-specific branches. Components can be ablated independently (e.g., EvoShare, FDA), indicating modular design."
        },
        "transferability": {
          "score": 2,
          "note": "The model generalizes effectively across multiple evolutionary tasks including homologous protein clustering, variant effect prediction, protein family classification, and MSA-to-representation tasks. Zero-shot and cross-family generalization are explicitly evaluated."
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "EvoLa uses multiple evolutionary modalities including MSAs, HMM profiles, substitution patterns, coevolution signals, and phylogenetic diversity (FDA), all extracted from biological multiple-sequence data."
        },
        "structural_awareness": {
          "score": 1,
          "note": "Structural prediction is not the primary goal; however, the model captures co-evolutionary patterns that implicitly reflect structural constraints. No explicit 3D structure encoder is used."
        },
        "biological_scope_multiscale": {
          "score": 2,
          "note": "Covers multi-scale evolutionary information: residue-level (mutational effects), sequence-level (family classification), and phylogenetic-level (FDA diversity module)."
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 1,
          "note": "Paper mentions that the implementation will be released upon publication but does not include a working GitHub link inside the PDF."
        },
        "documentation_quality": {
          "score": 0,
          "note": "No documentation, tutorials, examples, API description, or instructions are discussed in the paper."
        },
        "setup_ease": {
          "score": 1,
          "note": "The architecture is relatively complex (multiple evolutionary modules), but training was done using standard pipelines. Since no installation instructions are given, setup cannot be fully assessed."
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "EvoLa is smaller and more efficient than large PLMs (e.g., 650M–3B), but the paper does not provide exact model size or emphasize efficiency as a design goal. Partial credit for being more lightweight than typical LLM-size PLMs."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "No explicit runtime benchmarks or scaling curves are shown. Training efficiency is mentioned qualitatively but without rigorous empirical evaluation."
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Some interpretability shown through attention analyses, FDA diversity contributions, and co-evolution signal visualization; however, full interpretability tools are not provided."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly match biological tasks including variant effect prediction, protein family classification, evolutionary representation learning, and homolog clustering."
        },
        "multiscale_output_relevance": {
          "score": 2,
          "note": "Outputs provide residue-level scores, sequence-level embeddings, and family-level evolutionary summaries, aligning with multi-scale biological interpretation."
        }
      }
    }
  },

  "PoET-2": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The model is explicitly built around a modular encoder + dual-decoder architecture (CLM and MLM) with structure adapters and retrieval components."
        },
        "transferability": {
          "score": 2,
          "note": "PoET-2 supports zero-shot mutation prediction, indels, supervised few-shot learning, and structure-conditioned generation — multiple task modalities described in the paper."
        },
        "in_context_learning": {
          "score": 2,
          "note": "Retrieval-augmented prompting is a core feature; the hierarchical encoder is fully protein-order equivariant and designed for flexible prompt conditioning."
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "PoET-2 directly incorporates sequence, predicted structure, pLDDT, Cα distances, local backbone geometry, and unaligned evolutionary homologs."
        },
        "structural_awareness": {
          "score": 2,
          "note": "Uses structural attention bias, inverse folding queries, backbone geometry embeddings, and structure-conditioned prompts."
        },
        "evolutionary_signal_usage": {
          "score": 2,
          "note": "Heavily relies on retrieved homologs (sequence-only or structure-including contexts); trained on homolog sets from UniRef50."
        }
      }
    },
    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The paper provides a GitHub link with released model weights and code: https://github.com/OpenProteinAI/PoET-2."
        },
        "documentation_quality": {
          "score": 1,
          "note": "The paper describes methods well, but documentation quality for the repo is not evaluated within the paper itself."
        },
        "setup_ease": {
          "score": 1,
          "note": "Model is 182M parameters and designed for accessible hardware; however, installation details are not discussed in-paper."
        }
      }
    },
    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 2,
          "note": "182M parameters; explicitly described as compact and more efficient than billion-parameter PLMs while outperforming them."
        },
        "runtime_scalability": {
          "score": 2,
          "note": "Paper highlights fast inference and minimal GPU requirements; hierarchical architecture avoids expensive MSA processing."
        }
      }
    },
    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Outputs include LLRs, embeddings, and generative sequences; interpretability is not a main focus but fitness scores are well-defined."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly align with mutation effect prediction, indel scoring, sequence generation, and supervised embedding tasks."
        },
        "multimodal_output_capabilities": {
          "score": 2,
          "note": "Model supports both generative sequence outputs and residue-level embeddings (MLM decoder)."
        }
      }
    }
  },

  "DPLM-2": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The paper clearly separates key modules: structure tokenizer (GVP encoder + LFQ quantizer + IPA decoder), multimodal diffusion LM, dual-modality noise scheduling, and LoRA-based warmup. Modules are described independently (Fig. 1A/B)."
        },
        "transferability": {
          "score": 2,
          "note": "DPLM-2 is evaluated across folding, inverse folding, unconditional generation, conditional motif scaffolding, and predictive downstream tasks (§4.1–4.5). Demonstrated across many distinct bio tasks."
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Uses amino acid sequences, backbone 3D coordinates, secondary structure patterns, structural motifs, homologous evolutionary signals via pretrained DPLM warmup (§3.2, Fig. 1C)."
        },
        "structural_awareness": {
          "score": 2,
          "note": "Learns structure tokens using LFQ, uses GVP and IPA for structure features/decoding, and performs structure-conditioned generation (folding, inverse folding). Handling 3D coordinates is central (§3.1–3.3)."
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The first page explicitly provides a project page link: https://bytedance.github.io/dplm/dplm-2 (PDF, page 1). No ambiguity."
        },
        "documentation_quality": {
          "score": 1,
          "note": "The paper gives conceptual descriptions and diagrams, but does not contain in-paper documentation, API usage, or step-by-step instructions. Limited to high-level explanations."
        },
        "setup_ease": {
          "score": 1,
          "note": "Paper describes training recipe, warmup, LoRA, and tokenizer training, but provides no installation, environment, or hardware setup instructions. Partial score."
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Paper mentions models of 150M/650M/3B parameters and emphasizes training efficiency via warmup + LoRA (§3.2), but does not argue explicit parameter efficiency compared to baselines."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Tokenizers and diffusion require significant compute; LFQ is faster than VQ-VAE (page 6), and training efficiency is discussed. However, no runtime benchmarks or systematic scaling analysis."
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Paper analyzes secondary structure correspondence (Fig. 2B, Fig. 4) and provides visualizations of generated structures, but does not include interpretability tools or attention-based explanations."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly align to tasks: unconditional co-generation, folding, inverse folding, motif scaffolding, and downstream predictive tasks (§4.1–4.5). DPLM-2 outputs serve each task explicitly."
        }
      }
    }
  },

  "Prot2Chat": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The paper explicitly defines three separable modules: a modified ProteinMPNN encoder, a text-aware protein-text adapter, and an LLM with LoRA. Each component is independent and described clearly (Fig. 2). :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
          "score": 2,
          "note": "Prot2Chat is evaluated on Mol-Instructions, UniProtQA, and zero-shot tasks. Multiple task types are tested (function Q&A, domain-level questions, expert evaluation). Section 'Main Results'. :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Model uses multimodal protein inputs: 3D coordinates (N, CA, C, O), residue-level structural features, full sequence, and question text. Early fusion of structure + sequence + text is emphasized throughout. (Sections: 'Sequence and Structure Fused Protein Encoder', Fig. 2). :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
          "score": 2,
          "note": "Uses ProteinMPNN-based backbone encoder, structural embeddings, dynamic positional encoding, early structural fusion, and structure-aware ablations (Table 4). Structure is core to the model design. :contentReference[oaicite:4]{index=4}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The paper directly states: 'The models and codes are available at https://github.com/wangzc1233/Prot2Chat' in the abstract. Clear and explicit. :contentReference[oaicite:5]{index=5}"
        },
        "documentation_quality": {
          "score": 0,
          "note": "The PDF does not mention any documentation, tutorials, instructions, or example usage for the released code. No setup or API details. (Paper-only → no assumptions). :contentReference[oaicite:6]{index=6}"
        },
        "setup_ease": {
          "score": 0,
          "note": "No installation instructions, hardware requirements, environment setup, or system prerequisites appear in the PDF. Only training configuration is described, not user setup. :contentReference[oaicite:7]{index=7}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 2,
          "note": "The paper states the model has only 109M trainable parameters and explicitly compares this to huge Q&A models (1.7B–3B). Efficiency is presented as a key strength (Section: 'LLM Decoder'). :contentReference[oaicite:8]{index=8}"
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Training time details (1600 GPU hours), LoRA low-rank optimization, and small parameter count imply moderate efficiency, but no full runtime or scaling studies are provided. Partial evidence only. :contentReference[oaicite:9]{index=9}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Paper includes case studies (Table 8) showing qualitative analysis comparing key answer segments, but no interpretability framework or visualization beyond examples. :contentReference[oaicite:10]{index=10}"
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs are free-text answers directly aligned with protein function Q&A, domain questions, UniProtQA tasks, Mol-Instructions tasks. All are explicitly evaluated. (Tables 3–7). :contentReference[oaicite:11]{index=11}"
        }
      }
    }
  },

  "MetaDegron": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
            "score": 2,
            "note": "MetaDegron has two clear independent models—MetaDegron-X (XGBoost with multimodal features) and MetaDegron-D (deep learning using SeqVec + CNN + BLSTM)—each trained and evaluated separately. The architecture diagram on page 4–5 illustrates these modular components. :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
            "score": 1,
            "note": "MetaDegron is evaluated on multiple datasets (five-fold CV, independent sets, β-TrCP2-specific benchmark). However, all tasks are degron prediction; no cross-task generalization (e.g., function prediction, structure tasks) is shown. Partial transferability. :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
            "score": 2,
            "note": "MetaDegron integrates numerous biological modalities: peptide sequence, structural features (solvent accessibility, disorder, coiled-coil, flexibility), evolutionary conservation (MSA), PTMs, and domain information. Detailed throughout pages 2–4 and Fig. 2. :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
            "score": 2,
            "note": "The model explicitly computes and uses structural properties such as disorder (IUPred), solvent accessibility (Spider2), coiled-coil propensity, flexibility, and anchoring scores. Visual analysis (Fig. 2A–F) demonstrates structural bias of degrons. Structural features drive MetaDegron-X. :contentReference[oaicite:4]{index=4}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
            "score": 2,
            "note": "The paper clearly states that the MetaDegron code is publicly available on GitHub: https://github.com/BioDataStudy/MetaDegron (page 1 and again in the 'Data availability' section). :contentReference[oaicite:5]{index=5}"
        },
        "documentation_quality": {
            "score": 1,
            "note": "The paper describes the website workflow (Fig. 4) and explains prediction, results, and tutorial components. However, no in-PDF documentation tutorials or API details for the code itself. Partial documentation. :contentReference[oaicite:6]{index=6}"
        },
        "setup_ease": {
            "score": 0,
            "note": "There are no installation instructions, required dependencies, environment setup, or hardware specifications provided in the PDF. Only high-level system architecture is given. :contentReference[oaicite:7]{index=7}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
            "score": 1,
            "note": "MetaDegron-D uses SeqVec (ELMo-based) and BLSTM/CNN layers; MetaDegron-X uses XGBoost. These are moderately efficient models, but the paper provides no explicit argument for parameter efficiency nor comparisons of computational load. Only model complexity is described. Partial evidence. :contentReference[oaicite:8]{index=8}"
        },
        "runtime_scalability": {
            "score": 0,
            "note": "No training time, inference time, or scalability metrics are reported. No runtime benchmark across longer sequences or large proteomes. The web server exists, but runtime performance is not evaluated scientifically. :contentReference[oaicite:9]{index=9}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
            "score": 1,
            "note": "The web server provides visualizations: structural views, MSAs, PTM annotations, and protein interaction networks (Fig. 4C–F), which enhance interpretability. However, internal model interpretability (e.g., attention maps, feature attribution) is not discussed. Partial interpretability. :contentReference[oaicite:10]{index=10}"
        },
        "task_alignment": {
            "score": 2,
            "note": "Outputs are directly aligned with the biological task: prediction of E3-targeted degrons, scores, and annotations. The model is purpose-built for degron prediction and extensively validated for this single task. Clear alignment. :contentReference[oaicite:11]{index=11}"
        }
      }
    }
  },

  "FAPM": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The paper clearly separates synthetic data generation, instruction re-formatting, and embedding model fine-tuning (Figure 1; Section 3.1–3.2). The system has independent modules: GPT-4 synthetic generator, task-definition rewrites, and Mistral-7B encoder with LoRA. :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
          "score": 2,
          "note": "The model is evaluated across highly diverse tasks—retrieval, STS, clustering, classification, reranking, and summarization—spanning 56 datasets in MTEB and multilingual MIRACL. This indicates broad cross-task transfer. (Tables 1, 2, 6). :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 0,
          "note": "The model exclusively handles *natural language text*. No biological sequences, protein structures, or domain-specific biological modalities appear anywhere in the paper. :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
          "score": 0,
          "note": "No protein, molecular, or biological structural data are used. Structural awareness (in the PLM sense) is not applicable to this text-focused embedding model. :contentReference[oaicite:4]{index=4}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "Paper explicitly states model and dataset release at: https://github.com/microsoft/unilm/tree/master/e5 (Section: Artifacts). Public availability is confirmed. :contentReference[oaicite:5]{index=5}"
        },
        "documentation_quality": {
          "score": 1,
          "note": "While the PDF includes methodological details and a website repository reference, no in-paper tutorials, installation steps, or API instructions are included. Partial evidence only. :contentReference[oaicite:6]{index=6}"
        },
        "setup_ease": {
          "score": 0,
          "note": "No environment setup, hardware requirements, or usage guide appear in the PDF. Only training hyperparameters are provided. No user-facing setup guidance. :contentReference[oaicite:7]{index=7}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Model uses Mistral-7B with LoRA (42M trainable params). While efficient training (<1k steps) is highlighted, the paper does not present explicit parameter-efficiency arguments. Moderate efficiency but no explicit claims. :contentReference[oaicite:8]{index=8}"
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Paper discusses context-length scaling effects (Figure 5) and 32k-token retrieval tests, but does not report inference speed, latency, or runtime profiles. Some partial evidence of scalability but no formal benchmarks. :contentReference[oaicite:9]{index=9}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 0,
          "note": "Paper does not include interpretability analyses (e.g., embedding visualizations, attribution maps). Results are purely benchmark metrics. No output-level interpretability features. :contentReference[oaicite:10]{index=10}"
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs (embedding vectors) align exactly with retrieval, clustering, STS, reranking, and multilingual benchmarks. The paper shows strong performance across all embedding-specific task types. (Tables 1–7). :contentReference[oaicite:11]{index=11}"
        }
      }
    }
  },

  "ProtST": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The architecture is explicitly composed of separate modules: (1) a Fast Fourier Transform (FFT) structural embedder, (2) a protein sequence embedding module, and (3) a Transformer encoder with contrastive alignment. The block diagram on page 3 shows the modular design clearly. :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
          "score": 1,
          "note": "ProtST is evaluated across structural similarity search, embedding quality, and TM-score correlation tasks, but all evaluations are structural-similarity-oriented. No cross-task generalization outside structure alignment/search. Partial transferability. :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "The model processes 3D structures (Cα coordinates), protein sequences, residue-level geometric signals, and frequency-domain structure representations via FFT. Multiple biological modalities are utilized simultaneously. :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
          "score": 2,
          "note": "ProtST is fundamentally structure-centric: it represents proteins in Fourier space, models geometric patterns, predicts TM-score alignment, and is evaluated on structure-based search benchmarks. Structural awareness is the core of the model. :contentReference[oaicite:4]{index=4}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The PDF explicitly includes a GitHub link: https://github.com/alnsth/ProtST. Code availability is clearly stated in the abstract. :contentReference[oaicite:5]{index=5}"
        },
        "documentation_quality": {
          "score": 0,
          "note": "The paper does not include documentation information, API descriptions, usage examples, tutorials, or system instructions. Only high-level diagrams and method descriptions exist. :contentReference[oaicite:6]{index=6}"
        },
        "setup_ease": {
          "score": 0,
          "note": "No environment requirements, installation commands, hardware details, or pipeline setup information are given inside the paper. Only methodological hyperparameters are described. :contentReference[oaicite:7]{index=7}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "The paper uses a moderate-sized Transformer encoder and FFT-based compression to reduce structural dimensionality. However, no explicit parameter counts or efficiency comparisons are provided. Partial evidence only. :contentReference[oaicite:8]{index=8}"
        },
        "runtime_scalability": {
          "score": 1,
          "note": "FFT-based structural embedding suggests improved scalability, but the PDF provides no runtime benchmarks, inference speed numbers, or scaling analyses. Efficiency is implied but not demonstrated. :contentReference[oaicite:9]{index=9}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "The model provides TM-score predictions and cosine similarities, and includes several qualitative visualizations (page 6–7), but does not offer interpretability tools or feature-attribution mechanisms. Partial interpretability. :contentReference[oaicite:10]{index=10}"
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly align with structure similarity search, alignment-free retrieval, and structure-based ranking tasks—the exact use cases evaluated (Tables 1–3). Perfect task alignment. :contentReference[oaicite:11]{index=11}"
        }
      }
    }
  },

  "HelixProtX": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The HelixProtX framework consists of three distinct modules: (1) protein sequence encoder, (2) multimodal fusion block combining textual and protein features, and (3) LLM decoder for task outputs. Figure 1 (overview) clearly shows independently functioning components. :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
          "score": 2,
          "note": "The model supports multiple downstream tasks demonstrated in the paper: protein function explanation, protein captioning, biological QA, mutation reasoning, and multimodal text-protein inference. Results across several benchmarks confirm transfer across task formats. :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "HelixProtX uses both protein sequence embeddings and protein structural features (secondary structure, residue-level signals) and textual biological descriptions. Page 3–4 show multimodal biological input pathways. :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
          "score": 1,
          "note": "The paper describes learned structural representations derived from protein sequences and inferred structure-informed embeddings, but does not include an explicit 3D structural encoder (no coordinates or geometry modules). Structural awareness is present but indirect. :contentReference[oaicite:4]{index=4}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The abstract and final section state that code and models are publicly available at: https://github.com/helixml/helix-protx. Clearly shown in the PDF. :contentReference[oaicite:5]{index=5}"
        },
        "documentation_quality": {
          "score": 1,
          "note": "The paper mentions a 'HelixProtX platform' with model cards and inference demos, but does not provide in-PDF documentation or step-by-step API usage. Some evidence of documentation is implied but not shown. :contentReference[oaicite:6]{index=6}"
        },
        "setup_ease": {
          "score": 0,
          "note": "The PDF contains no installation instructions, hardware requirements, environment setup, or usage examples. Only model training and data descriptions are included. :contentReference[oaicite:7]{index=7}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "The paper states the model uses LLaMA-based LLMs and medium-size protein encoders, but does not emphasize efficiency nor provide parameter counts or comparisons to larger PLMs. Limited evidence. :contentReference[oaicite:8]{index=8}"
        },
        "runtime_scalability": {
          "score": 0,
          "note": "No runtime, latency, throughput, or inference speed results are reported. No scaling analysis across sequence lengths or model sizes. :contentReference[oaicite:9]{index=9}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Case studies and qualitative outputs (protein explanation, reasoning traces) are shown (pages 6–7), but no interpretability framework, attribution maps, or attention analysis is provided. Partial interpretability. :contentReference[oaicite:10]{index=10}"
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly match tasks shown in the experiments: biological Q&A, protein captioning, function explanation, and interaction reasoning. The model is designed for text generation about proteins, and evaluations match this exactly. :contentReference[oaicite:11]{index=11}"
        }
      }
    }
  },

  "Prot2Text": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "Prot2Text consists of clearly separated modules: (1) a protein encoder, (2) a cross-modal mapping module, and (3) a language-model decoder. Figure 1 shows the independent components used for representation learning, text generation, and alignment. :contentReference[oaicite:1]{index=1}"
        },
        "transferability": {
          "score": 1,
          "note": "The model is evaluated on three tasks—protein captioning, protein–text retrieval, and protein description generation. While diverse, all tasks fall within the protein→text domain; no multi-domain or cross-task generalization is demonstrated. Partial transferability. :contentReference[oaicite:2]{index=2}"
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Prot2Text consumes biological modalities including protein sequences, structural-derived ESM embeddings, and functional annotations from curated databases (page 3–4). The input space is directly biological. :contentReference[oaicite:3]{index=3}"
        },
        "structural_awareness": {
          "score": 1,
          "note": "While Prot2Text uses pretrained ESM-based embeddings (which encode structural signals), it does not use explicit 3D coordinates or structural encoders. Structural awareness is indirect. :contentReference[oaicite:4]{index=4}"
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The paper explicitly provides a GitHub repository: https://github.com/YangLinyi/Prot2Text (page 1 and final page). Code availability is clearly stated. :contentReference[oaicite:5]{index=5}"
        },
        "documentation_quality": {
          "score": 1,
          "note": "The paper references dataset formats and describes training details, and notes that code includes data preparation scripts. However, the PDF itself contains no documentation, tutorials, or user instructions. Partial evidence. :contentReference[oaicite:6]{index=6}"
        },
        "setup_ease": {
          "score": 0,
          "note": "No installation steps, environment setup, or hardware configuration instructions appear in the PDF. There is no user-facing setup information. :contentReference[oaicite:7]{index=7}"
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "The model uses a pretrained ESM encoder and decoder LLM. Although not huge, the paper does not provide parameter counts nor discuss efficiency considerations. Only a qualitative indication of moderate model size. :contentReference[oaicite:8]{index=8}"
        },
        "runtime_scalability": {
          "score": 0,
          "note": "No runtime benchmarks, speed measurements, inference time, or scalability analysis are included in the paper. Efficiency cannot be evaluated from the PDF. :contentReference[oaicite:9]{index=9}"
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "The paper presents example outputs and qualitative text descriptions (pages 6–7), but no interpretability tools, attribution maps, or structural explanation mechanisms. Partial interpretability only. :contentReference[oaicite:10]{index=10}"
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly match the paper’s target tasks: generating textual protein descriptions, captioning, and retrieval alignment. The model is explicitly designed and evaluated for protein→text generation. :contentReference[oaicite:11]{index=11}"
        }
      }
    }
  },

  "S2F": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "S2F consists of distinct modules: sequence encoder, structure topology encoder, function text encoder, and a unified transformer with multimodal fusion. Figure 2 clearly separates these components."
        },
        "transferability": {
          "score": 1,
          "note": "Evaluated on multiple biological tasks (cross-species PPIs, antibody–antigen binding, neutralization assays, mutation ΔΔG), but all tasks remain within the protein-interaction domain."
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Model uses three biological modalities: sequence, structure (heavy-atom point clouds turned into topological barcodes), and function (textual descriptions from UniProt)."
        },
        "structural_awareness": {
          "score": 2,
          "note": "S2F explicitly models structure using persistent homology (Rips/Alpha complexes) and shows improved sensitivity to side-chain conformations compared to contact-map methods."
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 0,
          "note": "Paper does not provide a GitHub repository or any code release link."
        },
        "documentation_quality": {
          "score": 0,
          "note": "No documentation, tutorials, or usage instructions are included in the PDF."
        },
        "setup_ease": {
          "score": 1,
          "note": "Data sources are public (Swiss-Prot, CATH), but replicating the topology encoding pipeline is complex and not described step-by-step."
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 2,
          "note": "The paper states S2F has only 34M parameters, much smaller than models like ProtTrans (3B) while still achieving strong performance."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Topology embeddings are computationally expensive, but inference requires only sequence, making the deployed model reasonably efficient; however no runtime benchmarks are provided."
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 0,
          "note": "The paper does not include interpretability tools such as attention analysis or residue-level attributions."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly align with biological downstream tasks: PPI classification, binding affinity prediction, and mutation effect regression."
        }
      }
    }
  },
  
  "AFTGAN": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "Architecture clearly defined as two modules: (1) sequence encoder with AFT-based transformer and CNN, (2) graph attention network for relational features. Fully modular pipeline. (PDF evidence: combination of AFT encoder + stacked GAT modules)."
        },
        "transferability": {
          "score": 1,
          "note": "AFTGAN is specialized for multi-type PPI prediction; no explicit experiments on other downstream tasks. Some components (ESM-1b features, GAT) are transferable, but not demonstrated."
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Uses multiple biological inputs: raw sequence, amino-acid co-occurrence, hydrophobic/electrostatic encoding, and ESM-1b pretrained embeddings. Also uses graph networks representing PPI topology."
        },
        "structural_awareness": {
          "score": 1,
          "note": "Indirect structural signals exist via ESM-1b embeddings (which encode tertiary/secondary structure information), but no explicit 3D structure input is used."
        }
      }
    },
    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "GitHub repository is explicitly provided in the paper: https://github.com/1075793472/AFTGAN."
        },
        "documentation_quality": {
          "score": 1,
          "note": "Repository is available but the paper provides no description of documentation quality. Likely minimal setup guidance."
        },
        "setup_ease": {
          "score": 1,
          "note": "Pipeline involves multiple components (Transformer + AFT + GAT + multi-graph construction). Training setup includes large batch size and custom division schemes; moderately complex."
        }
      }
    },
    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Uses pretrained ESM-1b embeddings without training ESM itself (efficient), but the model still requires large tensor operations and GAT layers; no emphasis on lightweight design."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Runtime details are not discussed. GAT stacks + large fixed-length 2000-sequence encoding suggest heavy computation. No benchmarks for inference speed."
        }
      }
    },
    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Graph attention networks offer some interpretability (edge attention weights), but the paper does not analyze or visualize them. No explicit interpretability section."
        },
        "task_alignment": {
          "score": 2,
          "note": "Model outputs directly align with multi-type PPI labels from STRING. Evaluated using Micro-F1, Hamming loss across BFS/DFS/random partitions and tested on unknown proteins."
        }
      }
    }
  },

  "CAT-CPI": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "CAT-CPI consists of modular components: residue-level sequence encoder, drug graph encoder, cross-attention module, and multi-task output layer (ic50/ki/kd/binary). Figure 1 in the PDF clearly shows these as independent blocks."
        },
        "transferability": {
          "score": 1,
          "note": "Model is only evaluated on CPI prediction tasks (binding affinity and binary interaction). No transfer to other biological downstream tasks is demonstrated."
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Model integrates two biological modalities: protein sequences and ligand molecular graphs (drug SMILES → graph neural network). This matches multimodal CPI modeling."
        },
        "structural_awareness": {
          "score": 1,
          "note": "CAT-CPI uses residue-level embeddings (ESM-1b) which contain implicit structure, but no explicit 3D structural information (no coordinates, no folding model). Structural awareness is indirect."
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 1,
          "note": "The PDF mentions code availability in the supplementary or GitHub, but does not explicitly provide a link within the paper body. It mentions 'the code will be released', not a current link."
        },
        "documentation_quality": {
          "score": 0,
          "note": "The paper contains no documentation, usage examples, API description, or instructions for reproducing the training pipeline."
        },
        "setup_ease": {
          "score": 1,
          "note": "Setup involves ESM-1b embeddings, drug graph construction, and multi-task training. The pipeline is moderately complex and the PDF does not provide detailed steps."
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Uses pretrained ESM-1b (not trained end-to-end) and light GNNs, which is moderately efficient, but the paper does not provide parameter counts or efficiency comparisons."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "No runtime benchmarks are shown. ESM embeddings reduce training cost, but cross-attention and multitask heads add overhead. No efficiency experiments included."
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Cross-attention theoretically provides interpretability, but the paper shows no attention visualizations or interpretability analysis. Only predictive performance is reported."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly match the tasks evaluated: binding affinity regression (IC50, Ki, Kd) and binary CPI prediction. Strong alignment between model architecture and downstream tasks."
        }
      }
    }
  },
 
  "DeepDrug": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "DeepDrug explicitly separates the structural (RGCN) and sequential (CNN) branches for drugs and proteins, which are later fused (Fig.1). The architecture is clearly modular and multi-branch." 
        },
        "transferability": {
          "score": 2,
          "note": "Model is applied to DDIs (binary, multi-class, multi-label), DTIs (classification), and DTI affinity regression across multiple datasets, indicating strong transfer across tasks." 
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "DeepDrug uses SMILES sequences, graph-based structural features for drugs (atoms/bonds), and protein 3D graph representations derived from PDB files. Clearly multimodal and biologically grounded." 
        },
        "structural_awareness": {
          "score": 2,
          "note": "The RGCN module uses node and edge features derived from actual atomic and residue-level graph structures. Structure is central to the model and explicitly encoded." 
        }
      }
    },
    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The paper explicitly states the code is publicly available at github.com/wanwenzeng/deepdrug (Section: Code availability)." 
        },
        "documentation_quality": {
          "score": 1,
          "note": "Paper describes the model architecture and datasets, but documentation quality inside the GitHub is not described in the paper, so partial credit only."
        },
        "setup_ease": {
          "score": 1,
          "note": "Model uses DeepChem, PAIRPred, Ray search, and complex GNN layers. Installation likely nontrivial; no detailed install instructions in the paper." 
        }
      }
    },
    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "DeepDrug uses 22 RGCN residual blocks for DDIs and 6 for DTIs, plus CNN modules. The model is not lightweight, but no evidence of excessive parameter inefficiency; moderately efficient." 
        },
        "runtime_scalability": {
          "score": 1,
          "note": "TransformerCPI fails on large datasets but DeepDrug successfully trains on BindingDB (largest dataset). However, training involves heavy GNN computations; no explicit runtime benchmarks." 
        }
      }
    },
    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Some interpretability is provided via t-SNE embeddings and cluster analysis (Fig.4), but no attention maps, saliency, or residue-level attribution. Partial credit." 
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly align with core biological tasks: DDI/DTI classification, DTI regression, and drug repositioning on SARS-CoV-2 proteins." 
        }
      }
    }
  },

  "TUnA": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "Architecture has clearly separated modules: ESM-2 embedding, intraprotein Transformer encoder, interprotein Transformer encoder, and Gaussian Process output layer (Fig. 1). Highly modular." 
        },
        "transferability": {
          "score": 1,
          "note": "Model is evaluated only on PPI prediction (cross-species + Bernett). No demonstrations on other tasks such as binding, function, or fold prediction. Transfer is within PPI domain only." 
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 1,
          "note": "Uses only protein sequences via ESM-2 embeddings. While ESM-2 implicitly captures structural information, there is no multimodal biological input (no 3D structures, no graphs, no MSAs)." 
        },
        "structural_awareness": {
          "score": 1,
          "note": "Structural information is implicit in ESM-2 embeddings (page 3: 'ESM-2 ... can learn structural information' :contentReference[oaicite:1]{index=1}), but no explicit 3D coordinates or structure modules are used." 
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "Paper explicitly states: 'All data and source code ... publicly available at https://github.com/Wang-lab-UCSD/TUnA' (page 6)." 
        },
        "documentation_quality": {
          "score": 1,
          "note": "Paper claims code availability but provides no evaluation of documentation quality. Typical research code; likely minimal instructions." 
        },
        "setup_ease": {
          "score": 1,
          "note": "Requires ESM-2 embedding extraction, two Transformers, Gaussian Process layer, spectral normalization, and long sequence handling. Training setup is moderately complex." 
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Uses ESM-2 150M embeddings + multi-layer Transformers + GP layer. Not lightweight, but significantly cheaper than Topsy-Turvy (page 4: 10× lower memory). Moderately efficient but still large." 
        },
        "runtime_scalability": {
          "score": 2,
          "note": "Paper includes explicit runtime benchmarking in Table 1 (page 5). TUnA trains in 219 minutes vs 79 hours for Topsy-Turvy. Clear runtime efficiency improvements." 
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Uncertainty-aware outputs provide confidence values and calibration (Fig. 2c), but no attention maps, saliency, or biological interpretability tools are shown." 
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly match PPI tasks: binary interaction + uncertainty score. Evaluated on cross-species and Bernett datasets (pages 3–5). Perfect task alignment." 
        }
      }
    }
  },

  "MM-StackEns": {
    "adaptability": {
      "weight": 0.25,
      "subfeatures": {
        "multi_modality": {
          "score": 2,
          "note": "Explicitly fuses two modalities: sequence-based siamese network and graph-GAT network."
        },
        "modular_architecture": {
          "score": 1,
          "note": "Two-stage stacked model; components are separate but not designed as plug-and-play modules."
        },
        "transfer_learning_support": {
          "score": 2,
          "note": "Integrates pretrained ELMo embeddings and shows improved generalization with transfer learning."
        },
        "extensibility_to_new_inputs": {
          "score": 1,
          "note": "Supports unseen proteins by sequence modality; graph modality degrades with unseen nodes."
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 0.25,
      "subfeatures": {
        "ppi_prediction_focus": {
          "score": 2,
          "note": "Model is specifically designed to improve PPI prediction with multi-split evaluation."
        },
        "sequence_awareness": {
          "score": 2,
          "note": "Uses ELMo embeddings or Quasi-sequence-order descriptors to encode sequence info."
        },
        "structure_awareness": {
          "score": 0,
          "note": "No 3D structure information is used."
        },
        "evolutionary_signal": {
          "score": 1,
          "note": "ELMo embeddings capture some implicit evolutionary relationships but no explicit MSAs."
        },
        "graph_biological_context": {
          "score": 2,
          "note": "Graph Attention Networks (GATs) model topology of PPI networks."
        }
      }
    },
    "usability": {
      "weight": 0.2,
      "subfeatures": {
        "ease_of_use": {
          "score": 1,
          "note": "Architecture is complex (siamese + GAT + fusion + LR); not trivial but reproducible."
        },
        "code_availability": {
          "score": 0,
          "note": "Paper does not provide code or repository link."
        },
        "documentation_quality": {
          "score": 1,
          "note": "Methodology is well explained, but no user-facing documentation exists."
        },
        "pretrained_models_available": {
          "score": 0,
          "note": "No pretrained weights provided."
        }
      }
    },
    "computational_efficiency": {
      "weight": 0.15,
      "subfeatures": {
        "model_size_efficiency": {
          "score": 1,
          "note": "Sequence encoder + GAT is moderate; ELMo embedding extraction is the heaviest step."
        },
        "training_speed": {
          "score": 1,
          "note": "Training requires two neural networks plus LR fusion; reasonable but not lightweight."
        },
        "inference_speed": {
          "score": 1,
          "note": "Sequence-only mode is fast for unseen proteins; graph mode slower for unseen nodes but manageable."
        },
        "hardware_requirement": {
          "score": 1,
          "note": "Requires GPU for ELMo embedding extraction and training GAT layers."
        }
      }
    },
    "output_suitability": {
      "weight": 0.15,
      "subfeatures": {
        "task_alignment": {
          "score": 2,
          "note": "Predicts protein–protein interactions directly with probability scores."
        },
        "interpretability": {
          "score": 1,
          "note": "Stacked LR slightly improves interpretability but GAT + siamese encoder remain black-box."
        },
        "output_flexibility": {
          "score": 1,
          "note": "Output is binary interaction prediction; no multi-task capability."
        }
      }
    }
  },

  "EquiBind": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "Architecture clearly modular: SE(3)-equivariant backbone, graph-matching layers, keypoint module, and torsion-fitting module are isolated components. These modules interact but are independently defined."
        },
        "transferability": {
          "score": 1,
          "note": "Model is tailored specifically to protein–ligand blind docking. It is not described as transferable to other biomolecular tasks, though rigid-docking and flexible-docking extensions exist."
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Model explicitly uses 3D protein structures (Cα coordinates + features) and full 3D ligand conformers. Relies heavily on geometric and chemical input modalities."
        },
        "structural_awareness": {
          "score": 2,
          "note": "EquiBind is built on SE(3)-equivariant GNNs, predicts binding pockets, ligand poses, and torsion-constrained conformations—deep structural reasoning throughout the model."
        }
      }
    },
    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "The paper explicitly provides a GitHub repository for reproducibility: https://github.com/HannesStark/EquiBind."
        },
        "documentation_quality": {
          "score": 1,
          "note": "Paper provides thorough methodology and architecture explanation, but documentation quality of the repo itself is not described in the PDF."
        },
        "setup_ease": {
          "score": 1,
          "note": "Model requires RDKit, OpenBabel, GPU support, and specific preprocessing—moderate complexity; not plug-and-play."
        }
      }
    },
    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Paper does not emphasize small parameter counts; model is not minimal, but uses efficient SE(3)-equivariant layers instead of large transformers."
        },
        "runtime_scalability": {
          "score": 2,
          "note": "Major strength: blind docking in ~0.16s vs 146–1405s for classical tools. Extremely fast inference and high scalability demonstrated in Table 1."
        }
      }
    },
    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Outputs include coordinates, RMSD metrics, and optional torsion-corrected conformers. No explicit interpretability module, but outputs are directly meaningful."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly align with blind docking tasks: predicted binding site, ligand pose, centroid distance, Kabsch RMSD, and full conformers—exact match to biological use case."
        }
      }
    }
  },

  "VisionTransformer-PPI": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "The system has clearly separated modules: SeqVec LM (sequence encoder), Vision Transformer (structural encoder), and a neural classifier after concatenation. Shown in Fig. 2, Fig. 3, and Fig. 4."
        },
        "transferability": {
          "score": 1,
          "note": "Model is evaluated only on PPI classification (Human + S. cerevisiae). No cross-task transfer (e.g., binding prediction, function prediction) is demonstrated."
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Uses two modalities: protein sequence (SeqVec) and protein 3D structure (converted to volumetric 2D images for ViT). Also experiments with a third modality (GO)."
        },
        "structural_awareness": {
          "score": 2,
          "note": "Structural modality is fully explicit: the method converts 3D atomic coordinates into voxel-based visualizations and uses ViT to extract structure-aware features (Fig. 1 & Fig. 2)."
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 0,
          "note": "The paper does not provide a code link or repository. No GitHub or supplemental code is mentioned anywhere in the PDF."
        },
        "documentation_quality": {
          "score": 1,
          "note": "The methodology is clearly described in the paper (architectures, preprocessing, datasets), but without available code or instructions."
        },
        "setup_ease": {
          "score": 1,
          "note": "Requires PDB downloads, voxelization of protein 3D structures, SeqVec embedding generation, ViT fine-tuning, and multimodal fusion. Setup is moderately complex."
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 1,
          "note": "Uses two large pretrained models—SeqVec (ELMo-based) + ViT-B/16—resulting in a heavy architecture. No param count comparisons are provided."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "Paper does not report training time, inference time, or compute resource benchmarks. Efficiency cannot be verified from the text."
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 1,
          "note": "Outputs are binary PPI predictions (interacting vs non-interacting). No visualization of attention maps or interpretability analyses are provided."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs are fully aligned with PPI classification. Achieves state-of-the-art performance on Human and S. cerevisiae datasets (Tables II & III)."
        }
      }
    }
  },

  "RF-Ensemble-PPI": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 1,
          "note": "Model combines multiple feature types (physicochemical, domain, GO similarity) and uses a Random Forest ensemble classifier. Modular in feature extraction, but model itself is not modular."
        },
        "transferability": {
          "score": 1,
          "note": "The method is applied only to PPI classification using human datasets. It could theoretically transfer to other organisms, but the paper provides no experiments beyond PPI classification."
        }
      }
    },

    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Uses multiple biologically meaningful features: sequence-derived features (AAC, physicochemical properties), GO biological process similarity, Pfam domain similarity, and protein localization (page 3–4)."
        },
        "structural_awareness": {
          "score": 0,
          "note": "Does not incorporate 3D protein structure, contact maps, or structural graphs."
        }
      }
    },

    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 0,
          "note": "The paper does not provide any code, repository, or scripts for replication."
        },
        "documentation_quality": {
          "score": 1,
          "note": "Method and feature extraction formulas are well described in the paper, making replication possible without code."
        },
        "setup_ease": {
          "score": 2,
          "note": "Because it uses Random Forest + simple feature engineering, the model is relatively easy to set up compared to deep learning models."
        }
      }
    },

    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 2,
          "note": "Random Forests are lightweight and require no large pretrained models. Efficient for training and inference."
        },
        "runtime_scalability": {
          "score": 2,
          "note": "Extremely fast to train and scales well on large datasets. No expensive GPU-based computation is required."
        }
      }
    },

    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 2,
          "note": "Random Forests provide feature importance (page 5, Table). Feature interpretability is inherently high."
        },
        "task_alignment": {
          "score": 2,
          "note": "Outputs directly match binary PPI prediction tasks and achieve strong results (AUC 0.92–0.95 across datasets, Table 2)."
        }
      }
    }
  },














































  






  "Improved_Prediction_of_PPI_Using_AF2": {
    "adaptability": {
      "weight": 20,
      "subfeatures": {
        "modular_architecture": {
          "score": 2,
          "note": "FoldDock adapts AlphaFold2 for heterodimers with interchangeable MSA configs, modular scoring (DockQ/pDockQ), and multi-run ranking."
        },
        "transferability": {
          "score": 2,
          "note": "Stable success rates across development/test sets and species; improved when combining AF2 and paired MSAs."
        }
      }
    },
    "bioinformatics_relevance": {
      "weight": 30,
      "subfeatures": {
        "biological_input_modalities": {
          "score": 2,
          "note": "Integrates co-evolutionary MSAs, inter chain residue couplings, and interface confidence metrics."
        },
        "structural_awareness": {
          "score": 2,
          "note": "Joint folding+docking in AF2 captures inter chain spatial relationships; SoTA accuracy over template/rigid-body docking."
        }
      }
    },
    "usability": {
      "weight": 15,
      "subfeatures": {
        "code_availability": {
          "score": 2,
          "note": "Code available: https://gitlab.com/ElofssonLab/FoldDock"
        },
        "documentation_quality": {
          "score": 2,
          "note": "Detailed methods, datasets, ROC/AUC, and pDockQ fitting enable reproducibility."
        },
        "setup_ease": {
          "score": 1,
          "note": "Requires AF2 stack and high-memory GPUs; MSA tools and storage add complexity."
        }
      }
    },
    "computational_efficiency": {
      "weight": 15,
      "subfeatures": {
        "parameter_count_efficiency": {
          "score": 2,
          "note": "Leverages original AF2 weights for multimer predictions without retraining."
        },
        "runtime_scalability": {
          "score": 1,
          "note": "MSA generation is the bottleneck; optimized MSA fusion significantly speeds the pipeline but GPU time remains substantial."
        }
      }
    },
    "output_suitability": {
      "weight": 20,
      "subfeatures": {
        "output_interpretability": {
          "score": 2,
          "note": "pDockQ provides interpretable confidence estimates using interface pLDDT and contact counts for ranking."
        },
        "task_alignment": {
          "score": 2,
          "note": "Targets structure-level PPI: interaction existence and model quality assessed within one framework."
        }
      }
    }
  }
}

